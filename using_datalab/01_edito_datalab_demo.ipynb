{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDITO Datalab Demo: STAC, Parquet, and Zarr\n",
        "\n",
        "This notebook demonstrates the core workflow of using EDITO Datalab:\n",
        "1. **Find services** on the datalab website\n",
        "2. **Configure services** (RStudio, Jupyter, VSCode)\n",
        "3. **Run analysis** with STAC search, Parquet reading, and Zarr data\n",
        "\n",
        "Perfect for a 15-minute tutorial! ğŸš€\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. STAC Search - Finding Marine Data\n",
        "\n",
        "First, let's search the EDITO STAC catalog to find available marine datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow as pa\n",
        "import xarray as xr\n",
        "import zarr\n",
        "import boto3\n",
        "import os\n",
        "import fsspec\n",
        "import s3fs\n",
        "\n",
        "print(\"ğŸŒŠ EDITO Datalab Jupyter Demo\")\n",
        "print(\"=\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to EDITO STAC API\n",
        "stac_endpoint = \"https://api.dive.edito.eu/data/\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(f\"{stac_endpoint}collections\")\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        collections = response.json()\n",
        "        \n",
        "        print(f\"âœ… Connected to EDITO STAC API\")\n",
        "        print(f\"Found {len(collections['collections'])} data collections\")\n",
        "        \n",
        "        # Show first few collections\n",
        "        print(\"\\nğŸ“‹ Available data collections:\")\n",
        "        for i, collection in enumerate(collections['collections'][:10]):\n",
        "            print(f\"{i+1:2d}. {collection['id']} - {collection.get('title', 'No title')}\")\n",
        "            \n",
        "        # Store available collection IDs for later use\n",
        "        available_collections = [col['id'] for col in collections['collections']]\n",
        "        print(f\"\\nğŸ’¡ Available collection IDs: {available_collections[:5]}...\")\n",
        "        \n",
        "    else:\n",
        "        print(f\"âŒ Failed to connect to EDITO API: HTTP {response.status_code}\")\n",
        "        print(f\"Response: {response.text}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error connecting to EDITO API: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search for biodiversity data\n",
        "print(\"\\nğŸ” Searching for biodiversity data...\")\n",
        "\n",
        "try:\n",
        "    search_url = f\"{stac_endpoint}search\"\n",
        "    \n",
        "    # Try to use an available collection, or search all collections\n",
        "    if 'available_collections' in locals() and available_collections:\n",
        "        # Look for occurrence data collection specifically (has parquet files)\n",
        "        occurrence_collections = [col for col in available_collections if 'occurrence' in col.lower() and 'emodnet-occurrence_data' in col]\n",
        "        if occurrence_collections:\n",
        "            search_collections = occurrence_collections[:1]  # Use occurrence data collection\n",
        "            print(f\"ğŸ” Searching in occurrence data collection: {search_collections[0]}\")\n",
        "        else:\n",
        "            # Look for other biodiversity collections\n",
        "            bio_collections = [col for col in available_collections if any(keyword in col.lower() for keyword in ['eurobis', 'bio', 'species', 'fish', 'habitat'])]\n",
        "            if bio_collections:\n",
        "                search_collections = bio_collections[:1]  # Use first biodiversity collection found\n",
        "                print(f\"ğŸ” Searching in biodiversity collection: {search_collections[0]}\")\n",
        "            else:\n",
        "                search_collections = available_collections[:1]  # Use first available collection\n",
        "                print(f\"ğŸ” Searching in available collection: {search_collections[0]}\")\n",
        "    else:\n",
        "        # Use the occurrence data collection as fallback\n",
        "        search_collections = [\"emodnet-occurrence_data\"]\n",
        "        print(\"ğŸ” Searching in occurrence data collection (fallback)\")\n",
        "    \n",
        "    search_params = {\n",
        "        \"collections\": search_collections,\n",
        "        \"limit\": 5\n",
        "    }\n",
        "    \n",
        "    response = requests.post(search_url, json=search_params)\n",
        "    \n",
        "    # Check if the response was successful\n",
        "    if response.status_code == 200:\n",
        "        search_results = response.json()\n",
        "        \n",
        "        # Check if the response contains features\n",
        "        if 'features' in search_results:\n",
        "            print(f\"âœ… Found {len(search_results['features'])} biodiversity items\")\n",
        "            \n",
        "            # Show first item\n",
        "            if search_results['features']:\n",
        "                first_item = search_results['features'][0]\n",
        "                print(f\"\\nğŸ“Š Sample item: {first_item['id']}\")\n",
        "                print(f\"Title: {first_item['properties'].get('title', 'No title')}\")\n",
        "                \n",
        "                print(\"\\nğŸ”— Available data formats:\")\n",
        "                for asset_name, asset in first_item['assets'].items():\n",
        "                    print(f\"- {asset_name}: {asset['href']}\")\n",
        "            else:\n",
        "                print(\"â„¹ï¸ No biodiversity items found in the search results\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ Unexpected response format: {search_results}\")\n",
        "    else:\n",
        "        print(f\"âŒ STAC search failed with status {response.status_code}\")\n",
        "        error_response = response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text\n",
        "        print(f\"Error details: {error_response}\")\n",
        "        \n",
        "        # Fallback: try a different collection or search without specific collection\n",
        "        print(\"\\nğŸ”„ Trying alternative search...\")\n",
        "        fallback_params = {\"limit\": 5}\n",
        "        fallback_response = requests.post(search_url, json=fallback_params)\n",
        "        \n",
        "        if fallback_response.status_code == 200:\n",
        "            fallback_results = fallback_response.json()\n",
        "            if 'features' in fallback_results and fallback_results['features']:\n",
        "                print(f\"âœ… Found {len(fallback_results['features'])} items in general search\")\n",
        "                first_item = fallback_results['features'][0]\n",
        "                print(f\"\\nğŸ“Š Sample item: {first_item['id']}\")\n",
        "                print(f\"Collection: {first_item.get('collection', 'Unknown')}\")\n",
        "            else:\n",
        "                print(\"â„¹ï¸ No items found in general search either\")\n",
        "        else:\n",
        "            print(f\"âŒ Fallback search also failed with status {fallback_response.status_code}\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error searching STAC: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Reading Parquet Data - Biodiversity Analysis\n",
        "\n",
        "Now let's read the biodiversity data using Parquet format for efficient access.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ“Š Querying parquet data metadata...\")\n",
        "\n",
        "# EUROBIS biodiversity occurrence data\n",
        "parquet_url = \"https://s3.waw3-1.cloudferro.com/emodnet/emodnet_biology/12639/eurobis_obisenv_view_2025-03-20.parquet\"\n",
        "\n",
        "print(f\"ğŸ”— Parquet URL: {parquet_url}\")\n",
        "\n",
        "try:\n",
        "    # Use fsspec to read parquet file directly from S3\n",
        "    print(\"ğŸ“¥ Reading parquet file using fsspec...\")\n",
        "    \n",
        "    # Create S3 filesystem with the correct endpoint\n",
        "    fs = s3fs.S3FileSystem(\n",
        "        endpoint_url=\"https://s3.waw3-1.cloudferro.com\",\n",
        "        anon=True  # Anonymous access\n",
        "    )\n",
        "    \n",
        "    # Parse the S3 path from the URL\n",
        "    s3_path = \"emodnet/emodnet_biology/12639/eurobis_obisenv_view_2025-03-20.parquet\"\n",
        "    \n",
        "    # Read parquet file metadata\n",
        "    parquet_file = pq.ParquetFile(s3_path, filesystem=fs)\n",
        "    \n",
        "    print(f\"âœ… Successfully connected to parquet file\")\n",
        "    print(f\"ğŸ“Š Number of row groups: {parquet_file.num_row_groups}\")\n",
        "    print(f\"ğŸ“ Total rows: {parquet_file.metadata.num_rows}\")\n",
        "    print(f\"ğŸ“‹ Schema (first 20 columns):\")\n",
        "    \n",
        "    # Print schema information\n",
        "    schema = parquet_file.schema\n",
        "    schema_fields = list(schema)\n",
        "    for i, field in enumerate(schema_fields[:20]):\n",
        "        print(f\"  {i+1:2d}. {field.name}: {field.physical_type}\")\n",
        "    \n",
        "    if len(schema_fields) > 20:\n",
        "        print(f\"  ... and {len(schema_fields) - 20} more columns\")\n",
        "    \n",
        "    # Get row group information (show only first few and last few)\n",
        "    print(f\"\\nğŸ“Š Row group details (showing first 5 and last 5):\")\n",
        "    total_groups = parquet_file.num_row_groups\n",
        "    for i in range(min(5, total_groups)):\n",
        "        rg = parquet_file.metadata.row_group(i)\n",
        "        print(f\"  Row group {i}: {rg.num_rows} rows, {rg.total_byte_size} bytes\")\n",
        "    \n",
        "    if total_groups > 10:\n",
        "        print(f\"  ... ({total_groups - 10} more row groups) ...\")\n",
        "        # Show last 5 row groups\n",
        "        start_idx = total_groups - 5\n",
        "        for i in range(start_idx, total_groups):\n",
        "            rg = parquet_file.metadata.row_group(i)\n",
        "            print(f\"  Row group {i}: {rg.num_rows} rows, {rg.total_byte_size} bytes\")\n",
        "    \n",
        "    # Read just a small sample to show data structure\n",
        "    print(f\"\\nğŸ“Š Reading sample data (first 10 rows)...\")\n",
        "    # Read only the first few columns and limit rows\n",
        "    sample_columns = list(schema.names)[:10]  # First 10 columns only\n",
        "    sample_table = parquet_file.read_row_groups([0], columns=sample_columns)\n",
        "    sample_df = sample_table.to_pandas().head(10)  # Only take first 10 rows\n",
        "    \n",
        "    print(f\"âœ… Sample data loaded: {len(sample_df)} rows\")\n",
        "    print(f\"ğŸ“‹ Sample columns: {list(sample_df.columns)}\")\n",
        "    print(f\"\\nFirst 3 rows of sample data:\")\n",
        "    print(sample_df.head(3))\n",
        "    \n",
        "    # Use the sample data for demonstration\n",
        "    df_sample = sample_df\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error reading parquet metadata: {e}\")\n",
        "    print(\"ğŸ“Š Creating sample biodiversity data for demonstration...\")\n",
        "    \n",
        "    # Create sample data for demonstration\n",
        "    df_sample = pd.DataFrame({\n",
        "        'scientificName': ['Scomber scombrus', 'Gadus morhua', 'Pleuronectes platessa', 'Merlangius merlangus', 'Solea solea'] * 20,\n",
        "        'decimalLatitude': np.random.uniform(50, 60, 100),\n",
        "        'decimalLongitude': np.random.uniform(0, 10, 100),\n",
        "        'eventDate': pd.date_range('2020-01-01', '2023-12-31', periods=100)\n",
        "    })\n",
        "    \n",
        "    print(f\"âœ… Created sample dataset with {len(df_sample)} records\")\n",
        "    print(f\"ğŸ“‹ Columns: {list(df_sample.columns)}\")\n",
        "    print(f\"ğŸ“Š Data types: {df_sample.dtypes.value_counts().to_dict()}\")\n",
        "    \n",
        "    # Show first few rows\n",
        "    print(f\"\\nFirst 5 rows:\")\n",
        "    print(df_sample.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter for marine species\n",
        "print(\"ğŸ  Analyzing marine species data...\")\n",
        "\n",
        "# Check what columns are available\n",
        "print(f\"ğŸ“‹ Available columns: {list(df_sample.columns)[:20]}...\")\n",
        "\n",
        "# Look for species-related columns\n",
        "species_columns = [col for col in df_sample.columns if any(keyword in col.lower() for keyword in ['species', 'scientific', 'taxon', 'name'])]\n",
        "print(f\"ğŸ” Species-related columns: {species_columns}\")\n",
        "\n",
        "if species_columns:\n",
        "    # Use the first species column found\n",
        "    species_col = species_columns[0]\n",
        "    print(f\"ğŸ“Š Using column '{species_col}' for species analysis\")\n",
        "    \n",
        "    # Show unique values in the species column\n",
        "    unique_species = df_sample[species_col].value_counts().head(10)\n",
        "    print(f\"\\nTop 10 species in '{species_col}':\")\n",
        "    print(unique_species)\n",
        "    \n",
        "    marine_data = df_sample  # Use all data for now\n",
        "else:\n",
        "    print(\"â„¹ï¸ No species columns found, using all data\")\n",
        "    marine_data = df_sample\n",
        "\n",
        "print(f\"âœ… Found {len(marine_data)} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple visualization\n",
        "if len(marine_data) > 0:\n",
        "    print(\"ğŸ“Š Creating visualizations...\")\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # Plot 1: Species distribution (if species column exists)\n",
        "    plt.subplot(2, 2, 1)\n",
        "    if species_columns:\n",
        "        species_count = marine_data[species_col].value_counts().head(5)\n",
        "        species_count.plot(kind='bar')\n",
        "        plt.title(f'Top 5 Species in {species_col}')\n",
        "        plt.xticks(rotation=45)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No species data available', ha='center', va='center')\n",
        "        plt.title('Species Distribution')\n",
        "    \n",
        "    # Plot 2: Geographic distribution (if coordinates exist)\n",
        "    plt.subplot(2, 2, 2)\n",
        "    coord_cols = [col for col in marine_data.columns if any(keyword in col.lower() for keyword in ['lat', 'lon', 'longitude', 'latitude'])]\n",
        "    if coord_cols and len(coord_cols) >= 2:\n",
        "        lat_col = [col for col in coord_cols if 'lat' in col.lower()][0]\n",
        "        lon_col = [col for col in coord_cols if 'lon' in col.lower()][0]\n",
        "        plt.scatter(marine_data[lon_col], marine_data[lat_col], alpha=0.6, s=20)\n",
        "        plt.xlabel('Longitude')\n",
        "        plt.ylabel('Latitude')\n",
        "        plt.title('Geographic Distribution')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No coordinate data available', ha='center', va='center')\n",
        "        plt.title('Geographic Distribution')\n",
        "    \n",
        "    # Plot 3: Temporal distribution (if date columns exist)\n",
        "    plt.subplot(2, 2, 3)\n",
        "    date_cols = [col for col in marine_data.columns if any(keyword in col.lower() for keyword in ['date', 'year', 'time'])]\n",
        "    if date_cols:\n",
        "        date_col = date_cols[0]\n",
        "        if 'year' in date_col.lower():\n",
        "            marine_data[date_col].value_counts().sort_index().plot(kind='line')\n",
        "        else:\n",
        "            # Try to extract year from date\n",
        "            try:\n",
        "                marine_data['year'] = pd.to_datetime(marine_data[date_col]).dt.year\n",
        "                marine_data['year'].value_counts().sort_index().plot(kind='line')\n",
        "            except:\n",
        "                marine_data[date_col].value_counts().head(10).plot(kind='bar')\n",
        "        plt.title(f'Records by {date_col}')\n",
        "        plt.xlabel(date_col)\n",
        "        plt.ylabel('Count')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No date data available', ha='center', va='center')\n",
        "        plt.title('Temporal Distribution')\n",
        "    \n",
        "    # Plot 4: Summary stats\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.text(0.1, 0.7, f'Total Records: {len(marine_data)}', fontsize=12)\n",
        "    if species_columns:\n",
        "        plt.text(0.1, 0.5, f'Unique Species: {marine_data[species_col].nunique()}', fontsize=12)\n",
        "    else:\n",
        "        plt.text(0.1, 0.5, f'Total Columns: {len(marine_data.columns)}', fontsize=12)\n",
        "    \n",
        "    if coord_cols and len(coord_cols) >= 2:\n",
        "        lat_col = [col for col in coord_cols if 'lat' in col.lower()][0]\n",
        "        lon_col = [col for col in coord_cols if 'lon' in col.lower()][0]\n",
        "        plt.text(0.1, 0.3, f'Latitude Range: {marine_data[lat_col].min():.1f} - {marine_data[lat_col].max():.1f}', fontsize=12)\n",
        "        plt.text(0.1, 0.1, f'Longitude Range: {marine_data[lon_col].min():.1f} - {marine_data[lon_col].max():.1f}', fontsize=12)\n",
        "    else:\n",
        "        plt.text(0.1, 0.3, f'Data Shape: {marine_data.shape}', fontsize=12)\n",
        "    \n",
        "    plt.title('Summary Statistics')\n",
        "    plt.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ No marine data to visualize\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Reading Zarr Data - Oceanographic Analysis\n",
        "\n",
        "Now let's work with Zarr data for oceanographic analysis using xarray.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ§Š Reading oceanographic data from Zarr...\")\n",
        "\n",
        "# Example Zarr URL (you would get this from STAC search)\n",
        "# For demo purposes, we'll create sample oceanographic data\n",
        "print(\"Creating sample oceanographic data for demonstration...\")\n",
        "\n",
        "# Create sample oceanographic data\n",
        "lats = np.linspace(50, 60, 50)\n",
        "lons = np.linspace(0, 10, 50)\n",
        "times = pd.date_range('2020-01-01', '2020-12-31', freq='D')\n",
        "depths = np.array([0, 10, 20, 50, 100, 200, 500, 1000])\n",
        "\n",
        "# Create temperature data with realistic patterns\n",
        "temp_data = np.random.normal(10, 2, (len(times), len(depths), len(lats), len(lons)))\n",
        "# Add seasonal variation\n",
        "seasonal = 5 * np.sin(2 * np.pi * np.arange(len(times)) / 365.25)\n",
        "temp_data += seasonal[:, np.newaxis, np.newaxis, np.newaxis]\n",
        "# Add depth variation\n",
        "temp_data += -0.01 * depths[np.newaxis, :, np.newaxis, np.newaxis]\n",
        "\n",
        "# Create xarray Dataset\n",
        "ds = xr.Dataset({\n",
        "    'temperature': (['time', 'depth', 'lat', 'lon'], temp_data),\n",
        "    'salinity': (['time', 'depth', 'lat', 'lon'], \n",
        "                 temp_data + np.random.normal(0, 0.5, temp_data.shape))\n",
        "}, coords={\n",
        "    'time': times,\n",
        "    'depth': depths,\n",
        "    'lat': lats,\n",
        "    'lon': lons\n",
        "})\n",
        "\n",
        "print(f\"âœ… Created oceanographic dataset\")\n",
        "print(f\"Dimensions: {ds.dims}\")\n",
        "print(f\"Variables: {list(ds.data_vars)}\")\n",
        "print(f\"Coordinates: {list(ds.coords)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the oceanographic data\n",
        "print(\"ğŸ“Š Analyzing oceanographic data...\")\n",
        "\n",
        "# Calculate mean temperature by depth\n",
        "mean_temp_by_depth = ds.temperature.mean(dim=['time', 'lat', 'lon'])\n",
        "\n",
        "# Calculate seasonal cycle\n",
        "seasonal_temp = ds.temperature.mean(dim=['depth', 'lat', 'lon'])\n",
        "\n",
        "# Create visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Temperature profile by depth\n",
        "axes[0, 0].plot(mean_temp_by_depth, -mean_temp_by_depth.depth)\n",
        "axes[0, 0].set_xlabel('Temperature (Â°C)')\n",
        "axes[0, 0].set_ylabel('Depth (m)')\n",
        "axes[0, 0].set_title('Mean Temperature Profile')\n",
        "axes[0, 0].grid(True)\n",
        "\n",
        "# Plot 2: Seasonal temperature cycle\n",
        "axes[0, 1].plot(seasonal_temp.time, seasonal_temp)\n",
        "axes[0, 1].set_xlabel('Date')\n",
        "axes[0, 1].set_ylabel('Temperature (Â°C)')\n",
        "axes[0, 1].set_title('Seasonal Temperature Cycle')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Plot 3: Temperature at surface\n",
        "surface_temp = ds.temperature.isel(depth=0, time=0)\n",
        "im = axes[1, 0].contourf(surface_temp.lon, surface_temp.lat, surface_temp, levels=20)\n",
        "axes[1, 0].set_xlabel('Longitude')\n",
        "axes[1, 0].set_ylabel('Latitude')\n",
        "axes[1, 0].set_title('Surface Temperature (Jan 1, 2020)')\n",
        "plt.colorbar(im, ax=axes[1, 0])\n",
        "\n",
        "# Plot 4: Temperature vs Salinity\n",
        "temp_flat = ds.temperature.values.flatten()\n",
        "sal_flat = ds.salinity.values.flatten()\n",
        "# Sample for plotting\n",
        "sample_idx = np.random.choice(len(temp_flat), 1000, replace=False)\n",
        "axes[1, 1].scatter(sal_flat[sample_idx], temp_flat[sample_idx], alpha=0.6, s=1)\n",
        "axes[1, 1].set_xlabel('Salinity')\n",
        "axes[1, 1].set_ylabel('Temperature (Â°C)')\n",
        "axes[1, 1].set_title('Temperature vs Salinity')\n",
        "axes[1, 1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Oceanographic analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Summary - EDITO Datalab Workflow\n",
        "\n",
        "This notebook demonstrated the core EDITO Datalab workflow:\n",
        "\n",
        "### ğŸ¯ Key Steps:\n",
        "1. **Find Services**: Go to [datalab.dive.edito.eu](https://datalab.dive.edito.eu/) and select a service\n",
        "2. **Configure Service**: Choose RStudio, Jupyter, or VSCode with appropriate resources\n",
        "3. **Run Analysis**: Use STAC to find data, Parquet for tabular data, Zarr for arrays\n",
        "\n",
        "### ğŸ› ï¸ Services Available:\n",
        "- **RStudio**: Perfect for statistical analysis and visualization\n",
        "- **Jupyter**: Ideal for machine learning and data exploration\n",
        "- **VSCode**: Great for larger projects with R and Python\n",
        "\n",
        "### ğŸ“Š Data Formats:\n",
        "- **STAC**: Find and discover marine datasets\n",
        "- **Parquet**: Efficient tabular data (biodiversity, observations)\n",
        "- **Zarr**: Cloud-optimized arrays (oceanographic, climate data)\n",
        "\n",
        "### ğŸš€ Next Steps:\n",
        "- Try the RStudio service for R-based analysis\n",
        "- Explore more datasets in the EDITO STAC catalog\n",
        "- Use personal storage to save your results\n",
        "\n",
        "**Happy analyzing! ğŸŒŠğŸ **\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Personal Storage - Connect and Transfer Data\n",
        "\n",
        "Now let's connect to your personal storage and transfer data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to personal storage\n",
        "print(\"ğŸ’¾ Connecting to personal storage...\")\n",
        "\n",
        "# Check if storage credentials are available\n",
        "if os.getenv(\"AWS_ACCESS_KEY_ID\"):\n",
        "    print(\"âœ… Personal storage credentials found!\")\n",
        "    \n",
        "    # Connect to EDITO's MinIO storage using environment variables\n",
        "    s3 = boto3.client(\n",
        "        \"s3\",\n",
        "        endpoint_url=f\"https://{os.getenv('AWS_S3_ENDPOINT')}\",\n",
        "        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
        "        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
        "        aws_session_token=os.getenv('AWS_SESSION_TOKEN'),\n",
        "        region_name=os.getenv('AWS_DEFAULT_REGION')\n",
        "    )\n",
        "    \n",
        "    print(\"âœ… Connected to personal storage!\")\n",
        "    \n",
        "    # List your buckets to verify connection\n",
        "    try:\n",
        "        response = s3.list_buckets()\n",
        "        print(f\"ğŸ“ Available buckets: {[bucket['Name'] for bucket in response['Buckets']]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Could not list buckets: {e}\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ No storage credentials found. Make sure you're running in EDITO Datalab.\")\n",
        "    print(\"ğŸ’¡ Your credentials are automatically available in EDITO services\")\n",
        "    print(\"ğŸ’¡ No need to go to project settings - they're already there!\")\n",
        "    \n",
        "    # For demo purposes, create a mock connection\n",
        "    print(\"Creating mock connection for demonstration...\")\n",
        "    s3 = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process and save data to personal storage\n",
        "print(\"ğŸ“Š Processing data for storage...\")\n",
        "\n",
        "if len(marine_data) > 0:\n",
        "    # Process the marine data - adapt to available columns\n",
        "    if species_columns and coord_cols and len(coord_cols) >= 2:\n",
        "        # Use the actual column names found\n",
        "        species_col = species_columns[0]\n",
        "        lat_col = [col for col in coord_cols if 'lat' in col.lower()][0]\n",
        "        lon_col = [col for col in coord_cols if 'lon' in col.lower()][0]\n",
        "        \n",
        "        processed_data = marine_data.groupby(species_col).agg({\n",
        "            lat_col: 'mean',\n",
        "            lon_col: 'mean',\n",
        "            species_col: 'count'\n",
        "        }).reset_index()\n",
        "        \n",
        "        processed_data.columns = ['species', 'mean_latitude', 'mean_longitude', 'count']\n",
        "    else:\n",
        "        # Create a simple summary if we don't have the expected columns\n",
        "        processed_data = pd.DataFrame({\n",
        "            'species': ['Sample Data'],\n",
        "            'mean_latitude': [marine_data.iloc[:, 0].mean() if len(marine_data.columns) > 0 else 0],\n",
        "            'mean_longitude': [marine_data.iloc[:, 1].mean() if len(marine_data.columns) > 1 else 0],\n",
        "            'count': [len(marine_data)]\n",
        "        })\n",
        "    \n",
        "    print(f\"âœ… Processed data: {len(processed_data)} species\")\n",
        "    print(processed_data.head())\n",
        "    \n",
        "    # Save to local file first\n",
        "    processed_data.to_csv('processed_marine_data.csv', index=False)\n",
        "    print(\"âœ… Data saved locally as processed_marine_data.csv\")\n",
        "    \n",
        "    # Upload to personal storage (if connected)\n",
        "    if s3:\n",
        "        try:\n",
        "            s3.put_object(\n",
        "                Bucket='your-bucket-name',  # Replace with your actual bucket name\n",
        "                Key='marine_analysis/processed_marine_data.csv',\n",
        "                Body=processed_data.to_csv(index=False),\n",
        "                ContentType='text/csv'\n",
        "            )\n",
        "            print(\"âœ… Data uploaded to personal storage!\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error uploading to storage: {e}\")\n",
        "            print(\"ğŸ’¡ Make sure to replace 'your-bucket-name' with your actual bucket name\")\n",
        "    else:\n",
        "        print(\"ğŸ’¡ To upload to storage, make sure you're running in EDITO Datalab\")\n",
        "        \n",
        "else:\n",
        "    print(\"âŒ No marine data to process\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download data from personal storage\n",
        "print(\"ğŸ“¥ Downloading data from personal storage...\")\n",
        "\n",
        "if s3:\n",
        "    try:\n",
        "        # Download from personal storage\n",
        "        response = s3.get_object(\n",
        "            Bucket='your-bucket-name',  # Replace with your actual bucket name\n",
        "            Key='marine_analysis/processed_marine_data.csv'\n",
        "        )\n",
        "        downloaded_data = pd.read_csv(response['Body'])\n",
        "        print(\"âœ… Data downloaded from personal storage!\")\n",
        "        print(f\"Downloaded {len(downloaded_data)} records\")\n",
        "        print(downloaded_data.head())\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error downloading from storage: {e}\")\n",
        "        print(\"ğŸ’¡ Make sure the file exists in your storage and bucket name is correct\")\n",
        "else:\n",
        "    print(\"ğŸ’¡ To download from storage, make sure you're running in EDITO Datalab\")\n",
        "    print(\"ğŸ’¡ Your credentials will be automatically available in EDITO services\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary - EDITO Datalab Workflow\n",
        "\n",
        "This notebook demonstrated the core EDITO Datalab workflow:\n",
        "\n",
        "### ğŸ¯ Key Steps:\n",
        "1. **Find Services**: Go to [datalab.dive.edito.eu](https://datalab.dive.edito.eu/) and select a service\n",
        "2. **Configure Service**: Choose RStudio, Jupyter, or VSCode with appropriate resources\n",
        "3. **Run Analysis**: Use STAC to find data, Parquet for tabular data, Zarr for arrays\n",
        "4. **Connect Storage**: Access your personal storage with automatic credentials\n",
        "5. **Process & Transfer**: Analyze data and save results to your storage\n",
        "\n",
        "### ğŸ› ï¸ Services Available:\n",
        "- **RStudio**: Perfect for statistical analysis and visualization\n",
        "- **Jupyter**: Ideal for machine learning and data exploration\n",
        "- **VSCode**: Great for larger projects with R and Python\n",
        "\n",
        "### ğŸ“Š Data Formats:\n",
        "- **STAC**: Find and discover marine datasets\n",
        "- **Parquet**: Efficient tabular data (biodiversity, observations)\n",
        "- **Zarr**: Cloud-optimized arrays (oceanographic, climate data)\n",
        "\n",
        "### ğŸš€ Next Steps:\n",
        "- Try the RStudio service for R-based analysis\n",
        "- Explore more datasets in the EDITO STAC catalog\n",
        "- Use personal storage to save your results\n",
        "\n",
        "**Happy analyzing! ğŸŒŠğŸ **\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
