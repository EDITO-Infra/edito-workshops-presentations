{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDITO Datalab Demo: STAC, Parquet, and Zarr\n",
        "\n",
        "This notebook demonstrates the core workflow of using EDITO Datalab:\n",
        "1. **Find services** on the datalab website\n",
        "2. **Configure services** (RStudio, Jupyter, VSCode)\n",
        "3. **Run analysis** with STAC search, Parquet reading, and Zarr data\n",
        "\n",
        "Perfect for a 15-minute tutorial! üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. STAC Search - Finding Marine Data\n",
        "\n",
        "First, let's search the EDITO STAC catalog to find available marine datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"üåä EDITO Datalab Jupyter Demo\")\n",
        "print(\"=\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to EDITO STAC API\n",
        "stac_endpoint = \"https://api.dive.edito.eu/data/\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(f\"{stac_endpoint}collections\")\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        collections = response.json()\n",
        "        \n",
        "        print(f\"‚úÖ Connected to EDITO STAC API\")\n",
        "        print(f\"Found {len(collections['collections'])} data collections\")\n",
        "        \n",
        "        # Show first few collections\n",
        "        print(\"\\nüìã Available data collections:\")\n",
        "        for i, collection in enumerate(collections['collections'][:10]):\n",
        "            print(f\"{i+1:2d}. {collection['id']} - {collection.get('title', 'No title')}\")\n",
        "            \n",
        "        # Store available collection IDs for later use\n",
        "        available_collections = [col['id'] for col in collections['collections']]\n",
        "        print(f\"\\nüí° Available collection IDs: {available_collections[:5]}...\")\n",
        "        \n",
        "    else:\n",
        "        print(f\"‚ùå Failed to connect to EDITO API: HTTP {response.status_code}\")\n",
        "        print(f\"Response: {response.text}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error connecting to EDITO API: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search for biodiversity data\n",
        "print(\"\\nüîç Searching for biodiversity data...\")\n",
        "\n",
        "try:\n",
        "    search_url = f\"{stac_endpoint}search\"\n",
        "    \n",
        "    # Try to use an available collection, or search all collections\n",
        "    if 'available_collections' in locals() and available_collections:\n",
        "        # Look for occurrence data collection specifically (has parquet files)\n",
        "        occurrence_collections = [col for col in available_collections if 'occurrence' in col.lower() and 'emodnet-occurrence_data' in col]\n",
        "        if occurrence_collections:\n",
        "            search_collections = occurrence_collections[:1]  # Use occurrence data collection\n",
        "            print(f\"üîç Searching in occurrence data collection: {search_collections[0]}\")\n",
        "        else:\n",
        "            # Look for other biodiversity collections\n",
        "            bio_collections = [col for col in available_collections if any(keyword in col.lower() for keyword in ['eurobis', 'bio', 'species', 'fish', 'habitat'])]\n",
        "            if bio_collections:\n",
        "                search_collections = bio_collections[:1]  # Use first biodiversity collection found\n",
        "                print(f\"üîç Searching in biodiversity collection: {search_collections[0]}\")\n",
        "            else:\n",
        "                search_collections = available_collections[:1]  # Use first available collection\n",
        "                print(f\"üîç Searching in available collection: {search_collections[0]}\")\n",
        "    else:\n",
        "        # Use the occurrence data collection as fallback\n",
        "        search_collections = [\"emodnet-occurrence_data\"]\n",
        "        print(\"üîç Searching in occurrence data collection (fallback)\")\n",
        "    \n",
        "    search_params = {\n",
        "        \"collections\": search_collections,\n",
        "        \"limit\": 5\n",
        "    }\n",
        "    \n",
        "    response = requests.post(search_url, json=search_params)\n",
        "    \n",
        "    # Check if the response was successful\n",
        "    if response.status_code == 200:\n",
        "        search_results = response.json()\n",
        "        \n",
        "        # Check if the response contains features\n",
        "        if 'features' in search_results:\n",
        "            print(f\"‚úÖ Found {len(search_results['features'])} biodiversity items\")\n",
        "            \n",
        "            # Show first item\n",
        "            if search_results['features']:\n",
        "                first_item = search_results['features'][0]\n",
        "                print(f\"\\nüìä Sample item: {first_item['id']}\")\n",
        "                print(f\"Title: {first_item['properties'].get('title', 'No title')}\")\n",
        "                \n",
        "                print(\"\\nüîó Available data formats:\")\n",
        "                for asset_name, asset in first_item['assets'].items():\n",
        "                    print(f\"- {asset_name}: {asset['href']}\")\n",
        "            else:\n",
        "                print(\"‚ÑπÔ∏è No biodiversity items found in the search results\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Unexpected response format: {search_results}\")\n",
        "    else:\n",
        "        print(f\"‚ùå STAC search failed with status {response.status_code}\")\n",
        "        error_response = response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text\n",
        "        print(f\"Error details: {error_response}\")\n",
        "        \n",
        "        # Fallback: try a different collection or search without specific collection\n",
        "        print(\"\\nüîÑ Trying alternative search...\")\n",
        "        fallback_params = {\"limit\": 5}\n",
        "        fallback_response = requests.post(search_url, json=fallback_params)\n",
        "        \n",
        "        if fallback_response.status_code == 200:\n",
        "            fallback_results = fallback_response.json()\n",
        "            if 'features' in fallback_results and fallback_results['features']:\n",
        "                print(f\"‚úÖ Found {len(fallback_results['features'])} items in general search\")\n",
        "                first_item = fallback_results['features'][0]\n",
        "                print(f\"\\nüìä Sample item: {first_item['id']}\")\n",
        "                print(f\"Collection: {first_item.get('collection', 'Unknown')}\")\n",
        "            else:\n",
        "                print(\"‚ÑπÔ∏è No items found in general search either\")\n",
        "        else:\n",
        "            print(f\"‚ùå Fallback search also failed with status {fallback_response.status_code}\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error searching STAC: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Reading Parquet Data - Biodiversity Analysis\n",
        "\n",
        "Now let's read the biodiversity data using Parquet format for efficient access.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyarrow.parquet as pq\n",
        "import pyarrow as pa\n",
        "\n",
        "print(\"üìä Reading biodiversity data from Parquet...\")\n",
        "\n",
        "# EUROBIS biodiversity occurrence data\n",
        "parquet_url = \"https://s3.waw3-1.cloudferro.com/emodnet/emodnet_biology/12639/eurobis_obisenv_view_2025-03-20.parquet\"\n",
        "\n",
        "try:\n",
        "    # Read a sample of the data (first 1000 records)\n",
        "    df = \n",
        "    df_sample = df.head(1000)\n",
        "    \n",
        "    print(f\"‚úÖ Loaded {len(df_sample)} biodiversity records (sample)\")\n",
        "    print(f\"üìã Total records in dataset: {len(df)}\")\n",
        "    print(f\"\\nColumns: {list(df_sample.columns)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error reading parquet: {e}\")\n",
        "    # Create sample data for demo\n",
        "    print(\"Creating sample data for demonstration...\")\n",
        "    df_sample = pd.DataFrame({\n",
        "        'scientificName': ['Scomber scombrus', 'Gadus morhua', 'Pleuronectes platessa'] * 100,\n",
        "        'decimalLatitude': np.random.uniform(50, 60, 300),\n",
        "        'decimalLongitude': np.random.uniform(0, 10, 300),\n",
        "        'eventDate': pd.date_range('2020-01-01', '2023-12-31', periods=300)\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter for marine species\n",
        "print(\"üê† Filtering for marine species...\")\n",
        "\n",
        "marine_keywords = ['fish', 'pisces', 'mollusca', 'algae', 'crustacea', 'crab', 'mollusk']\n",
        "\n",
        "if 'scientificName' in df_sample.columns:\n",
        "    marine_mask = df_sample['scientificName'].str.contains('|'.join(marine_keywords), case=False, na=False)\n",
        "    marine_data = df_sample[marine_mask]\n",
        "else:\n",
        "    marine_data = df_sample  # Use sample data\n",
        "\n",
        "print(f\"‚úÖ Found {len(marine_data)} marine species records\")\n",
        "\n",
        "# Show top species\n",
        "if len(marine_data) > 0:\n",
        "    species_count = marine_data['scientificName'].value_counts().head(10)\n",
        "    print(\"\\nTop 10 marine species:\")\n",
        "    print(species_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple visualization\n",
        "if len(marine_data) > 0:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # Plot 1: Species distribution\n",
        "    plt.subplot(2, 2, 1)\n",
        "    species_count.head(5).plot(kind='bar')\n",
        "    plt.title('Top 5 Marine Species')\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    # Plot 2: Geographic distribution\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.scatter(marine_data['decimalLongitude'], marine_data['decimalLatitude'], \n",
        "               alpha=0.6, s=20)\n",
        "    plt.xlabel('Longitude')\n",
        "    plt.ylabel('Latitude')\n",
        "    plt.title('Geographic Distribution')\n",
        "    \n",
        "    # Plot 3: Temporal distribution\n",
        "    plt.subplot(2, 2, 3)\n",
        "    if 'eventDate' in marine_data.columns:\n",
        "        marine_data['year'] = pd.to_datetime(marine_data['eventDate']).dt.year\n",
        "        marine_data['year'].value_counts().sort_index().plot(kind='line')\n",
        "        plt.title('Records by Year')\n",
        "        plt.xlabel('Year')\n",
        "        plt.ylabel('Count')\n",
        "    \n",
        "    # Plot 4: Summary stats\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.text(0.1, 0.7, f'Total Records: {len(marine_data)}', fontsize=12)\n",
        "    plt.text(0.1, 0.5, f'Unique Species: {marine_data[\"scientificName\"].nunique()}', fontsize=12)\n",
        "    plt.text(0.1, 0.3, f'Latitude Range: {marine_data[\"decimalLatitude\"].min():.1f} - {marine_data[\"decimalLatitude\"].max():.1f}', fontsize=12)\n",
        "    plt.text(0.1, 0.1, f'Longitude Range: {marine_data[\"decimalLongitude\"].min():.1f} - {marine_data[\"decimalLongitude\"].max():.1f}', fontsize=12)\n",
        "    plt.title('Summary Statistics')\n",
        "    plt.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No marine data to visualize\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Reading Zarr Data - Oceanographic Analysis\n",
        "\n",
        "Now let's work with Zarr data for oceanographic analysis using xarray.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import zarr\n",
        "\n",
        "print(\"üßä Reading oceanographic data from Zarr...\")\n",
        "\n",
        "# Example Zarr URL (you would get this from STAC search)\n",
        "# For demo purposes, we'll create sample oceanographic data\n",
        "print(\"Creating sample oceanographic data for demonstration...\")\n",
        "\n",
        "# Create sample oceanographic data\n",
        "lats = np.linspace(50, 60, 50)\n",
        "lons = np.linspace(0, 10, 50)\n",
        "times = pd.date_range('2020-01-01', '2020-12-31', freq='D')\n",
        "depths = np.array([0, 10, 20, 50, 100, 200, 500, 1000])\n",
        "\n",
        "# Create temperature data with realistic patterns\n",
        "temp_data = np.random.normal(10, 2, (len(times), len(depths), len(lats), len(lons)))\n",
        "# Add seasonal variation\n",
        "seasonal = 5 * np.sin(2 * np.pi * np.arange(len(times)) / 365.25)\n",
        "temp_data += seasonal[:, np.newaxis, np.newaxis, np.newaxis]\n",
        "# Add depth variation\n",
        "temp_data += -0.01 * depths[np.newaxis, :, np.newaxis, np.newaxis]\n",
        "\n",
        "# Create xarray Dataset\n",
        "ds = xr.Dataset({\n",
        "    'temperature': (['time', 'depth', 'lat', 'lon'], temp_data),\n",
        "    'salinity': (['time', 'depth', 'lat', 'lon'], \n",
        "                 temp_data + np.random.normal(0, 0.5, temp_data.shape))\n",
        "}, coords={\n",
        "    'time': times,\n",
        "    'depth': depths,\n",
        "    'lat': lats,\n",
        "    'lon': lons\n",
        "})\n",
        "\n",
        "print(f\"‚úÖ Created oceanographic dataset\")\n",
        "print(f\"Dimensions: {ds.dims}\")\n",
        "print(f\"Variables: {list(ds.data_vars)}\")\n",
        "print(f\"Coordinates: {list(ds.coords)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the oceanographic data\n",
        "print(\"üìä Analyzing oceanographic data...\")\n",
        "\n",
        "# Calculate mean temperature by depth\n",
        "mean_temp_by_depth = ds.temperature.mean(dim=['time', 'lat', 'lon'])\n",
        "\n",
        "# Calculate seasonal cycle\n",
        "seasonal_temp = ds.temperature.mean(dim=['depth', 'lat', 'lon'])\n",
        "\n",
        "# Create visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Temperature profile by depth\n",
        "axes[0, 0].plot(mean_temp_by_depth, -mean_temp_by_depth.depth)\n",
        "axes[0, 0].set_xlabel('Temperature (¬∞C)')\n",
        "axes[0, 0].set_ylabel('Depth (m)')\n",
        "axes[0, 0].set_title('Mean Temperature Profile')\n",
        "axes[0, 0].grid(True)\n",
        "\n",
        "# Plot 2: Seasonal temperature cycle\n",
        "axes[0, 1].plot(seasonal_temp.time, seasonal_temp)\n",
        "axes[0, 1].set_xlabel('Date')\n",
        "axes[0, 1].set_ylabel('Temperature (¬∞C)')\n",
        "axes[0, 1].set_title('Seasonal Temperature Cycle')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Plot 3: Temperature at surface\n",
        "surface_temp = ds.temperature.isel(depth=0, time=0)\n",
        "im = axes[1, 0].contourf(surface_temp.lon, surface_temp.lat, surface_temp, levels=20)\n",
        "axes[1, 0].set_xlabel('Longitude')\n",
        "axes[1, 0].set_ylabel('Latitude')\n",
        "axes[1, 0].set_title('Surface Temperature (Jan 1, 2020)')\n",
        "plt.colorbar(im, ax=axes[1, 0])\n",
        "\n",
        "# Plot 4: Temperature vs Salinity\n",
        "temp_flat = ds.temperature.values.flatten()\n",
        "sal_flat = ds.salinity.values.flatten()\n",
        "# Sample for plotting\n",
        "sample_idx = np.random.choice(len(temp_flat), 1000, replace=False)\n",
        "axes[1, 1].scatter(sal_flat[sample_idx], temp_flat[sample_idx], alpha=0.6, s=1)\n",
        "axes[1, 1].set_xlabel('Salinity')\n",
        "axes[1, 1].set_ylabel('Temperature (¬∞C)')\n",
        "axes[1, 1].set_title('Temperature vs Salinity')\n",
        "axes[1, 1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Oceanographic analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Summary - EDITO Datalab Workflow\n",
        "\n",
        "This notebook demonstrated the core EDITO Datalab workflow:\n",
        "\n",
        "### üéØ Key Steps:\n",
        "1. **Find Services**: Go to [datalab.dive.edito.eu](https://datalab.dive.edito.eu/) and select a service\n",
        "2. **Configure Service**: Choose RStudio, Jupyter, or VSCode with appropriate resources\n",
        "3. **Run Analysis**: Use STAC to find data, Parquet for tabular data, Zarr for arrays\n",
        "\n",
        "### üõ†Ô∏è Services Available:\n",
        "- **RStudio**: Perfect for statistical analysis and visualization\n",
        "- **Jupyter**: Ideal for machine learning and data exploration\n",
        "- **VSCode**: Great for larger projects with R and Python\n",
        "\n",
        "### üìä Data Formats:\n",
        "- **STAC**: Find and discover marine datasets\n",
        "- **Parquet**: Efficient tabular data (biodiversity, observations)\n",
        "- **Zarr**: Cloud-optimized arrays (oceanographic, climate data)\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "- Try the RStudio service for R-based analysis\n",
        "- Explore more datasets in the EDITO STAC catalog\n",
        "- Use personal storage to save your results\n",
        "\n",
        "**Happy analyzing! üåäüê†**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Personal Storage - Connect and Transfer Data\n",
        "\n",
        "Now let's connect to your personal storage and transfer data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to personal storage\n",
        "print(\"üíæ Connecting to personal storage...\")\n",
        "\n",
        "import boto3\n",
        "import os\n",
        "\n",
        "# Check if storage credentials are available\n",
        "if os.getenv(\"AWS_ACCESS_KEY_ID\"):\n",
        "    print(\"‚úÖ Personal storage credentials found!\")\n",
        "    \n",
        "    # Connect to EDITO's MinIO storage using environment variables\n",
        "    s3 = boto3.client(\n",
        "        \"s3\",\n",
        "        endpoint_url=f\"https://{os.getenv('AWS_S3_ENDPOINT')}\",\n",
        "        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
        "        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
        "        aws_session_token=os.getenv('AWS_SESSION_TOKEN'),\n",
        "        region_name=os.getenv('AWS_DEFAULT_REGION')\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Connected to personal storage!\")\n",
        "    \n",
        "    # List your buckets to verify connection\n",
        "    try:\n",
        "        response = s3.list_buckets()\n",
        "        print(f\"üìÅ Available buckets: {[bucket['Name'] for bucket in response['Buckets']]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not list buckets: {e}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No storage credentials found. Make sure you're running in EDITO Datalab.\")\n",
        "    print(\"üí° Your credentials are automatically available in EDITO services\")\n",
        "    print(\"üí° No need to go to project settings - they're already there!\")\n",
        "    \n",
        "    # For demo purposes, create a mock connection\n",
        "    print(\"Creating mock connection for demonstration...\")\n",
        "    s3 = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process and save data to personal storage\n",
        "print(\"üìä Processing data for storage...\")\n",
        "\n",
        "if len(marine_data) > 0:\n",
        "    # Process the marine data\n",
        "    processed_data = marine_data.groupby('scientificName').agg({\n",
        "        'decimalLatitude': 'mean',\n",
        "        'decimalLongitude': 'mean',\n",
        "        'eventDate': 'count'\n",
        "    }).reset_index()\n",
        "    \n",
        "    processed_data.columns = ['species', 'mean_latitude', 'mean_longitude', 'count']\n",
        "    \n",
        "    print(f\"‚úÖ Processed data: {len(processed_data)} species\")\n",
        "    print(processed_data.head())\n",
        "    \n",
        "    # Save to local file first\n",
        "    processed_data.to_csv('processed_marine_data.csv', index=False)\n",
        "    print(\"‚úÖ Data saved locally as processed_marine_data.csv\")\n",
        "    \n",
        "    # Upload to personal storage (if connected)\n",
        "    if s3:\n",
        "        try:\n",
        "            s3.put_object(\n",
        "                Bucket='your-bucket-name',  # Replace with your actual bucket name\n",
        "                Key='marine_analysis/processed_marine_data.csv',\n",
        "                Body=processed_data.to_csv(index=False),\n",
        "                ContentType='text/csv'\n",
        "            )\n",
        "            print(\"‚úÖ Data uploaded to personal storage!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error uploading to storage: {e}\")\n",
        "            print(\"üí° Make sure to replace 'your-bucket-name' with your actual bucket name\")\n",
        "    else:\n",
        "        print(\"üí° To upload to storage, make sure you're running in EDITO Datalab\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ùå No marine data to process\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download data from personal storage\n",
        "print(\"üì• Downloading data from personal storage...\")\n",
        "\n",
        "if s3:\n",
        "    try:\n",
        "        # Download from personal storage\n",
        "        response = s3.get_object(\n",
        "            Bucket='your-bucket-name',  # Replace with your actual bucket name\n",
        "            Key='marine_analysis/processed_marine_data.csv'\n",
        "        )\n",
        "        downloaded_data = pd.read_csv(response['Body'])\n",
        "        print(\"‚úÖ Data downloaded from personal storage!\")\n",
        "        print(f\"Downloaded {len(downloaded_data)} records\")\n",
        "        print(downloaded_data.head())\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error downloading from storage: {e}\")\n",
        "        print(\"üí° Make sure the file exists in your storage and bucket name is correct\")\n",
        "else:\n",
        "    print(\"üí° To download from storage, make sure you're running in EDITO Datalab\")\n",
        "    print(\"üí° Your credentials will be automatically available in EDITO services\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary - EDITO Datalab Workflow\n",
        "\n",
        "This notebook demonstrated the core EDITO Datalab workflow:\n",
        "\n",
        "### üéØ Key Steps:\n",
        "1. **Find Services**: Go to [datalab.dive.edito.eu](https://datalab.dive.edito.eu/) and select a service\n",
        "2. **Configure Service**: Choose RStudio, Jupyter, or VSCode with appropriate resources\n",
        "3. **Run Analysis**: Use STAC to find data, Parquet for tabular data, Zarr for arrays\n",
        "4. **Connect Storage**: Access your personal storage with automatic credentials\n",
        "5. **Process & Transfer**: Analyze data and save results to your storage\n",
        "\n",
        "### üõ†Ô∏è Services Available:\n",
        "- **RStudio**: Perfect for statistical analysis and visualization\n",
        "- **Jupyter**: Ideal for machine learning and data exploration\n",
        "- **VSCode**: Great for larger projects with R and Python\n",
        "\n",
        "### üìä Data Formats:\n",
        "- **STAC**: Find and discover marine datasets\n",
        "- **Parquet**: Efficient tabular data (biodiversity, observations)\n",
        "- **Zarr**: Cloud-optimized arrays (oceanographic, climate data)\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "- Try the RStudio service for R-based analysis\n",
        "- Explore more datasets in the EDITO STAC catalog\n",
        "- Use personal storage to save your results\n",
        "\n",
        "**Happy analyzing! üåäüê†**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
